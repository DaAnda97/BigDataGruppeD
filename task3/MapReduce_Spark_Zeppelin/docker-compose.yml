version: "3.7"

services:
  
  master:
    image: bde2020/hadoop-namenode:1.2.1-hadoop2.8-java8
    container_name: master
    ports:
      - 9870:9870
      - 9000:9000
    volumes:
      - ./data/master:/hadoop/dfs/name
    environment:
      - CLUSTER_NAME=bigdata
    env_file:
      - ./hadoop.env
    networks: 
      - hadoop_network

  node01:
    image: bde2020/hadoop-datanode:1.2.1-hadoop2.8-java8
    container_name: node01
    restart: always
    volumes:
      - ./data/node01:/hadoop/dfs/data
    environment:
      SERVICE_PRECONDITION: "master:9870"
    env_file:
      - ./hadoop.env
    networks:
      - hadoop_network
  
  node02:
    image: bde2020/hadoop-datanode:1.2.1-hadoop2.8-java8
    container_name: node02
    volumes:
      - ./data/node02:/hadoop/dfs/data
    environment:
      SERVICE_PRECONDITION: "master:9870"
    env_file:
      - ./hadoop.env
    networks:
      - hadoop_network

  node03:
    image: bde2020/hadoop-datanode:1.2.1-hadoop2.8-java8
    container_name: node03
    volumes:
      - ./data/node03:/hadoop/dfs/data
    environment:
      SERVICE_PRECONDITION: "master:9870"
    env_file:
      - ./hadoop.env
    networks:
      - hadoop_network
    
  spark-master:
    image: bde2020/spark-master:2.3.1-hadoop2.8
    container_name: spark-master
    ports:
      - 8080:8080
      - 7077:7077
    environment:
      - "CORE_CONF_fs_defaultFS=hdfs://master:9870"
    depends_on:
      - master
      - node01
      - node02
      - node03
    networks:
      - hadoop_network

  spark-worker:
    image: bde2020/spark-worker:2.3.1-hadoop2.8
    container_name: spark-worker
    environment:
      - "SPARK_MASTER=spark://spark-master:7077"
      - "CORE_CONF_fs_defaultFS=hdfs://master:9870"
    depends_on:
      - spark-master
    networks:
      - hadoop_network
      
  zeppelin:
    image: bde2020/zeppelin:0.8.0-hadoop-2.8.0-spark-2.3.1
    container_name: zeppelin
    ports:
      - 80:8080
    volumes:
      - ./notebooks:/opt/zeppelin/notebook
    environment:
      - "CORE_CONF_fs_defaultFS=hdfs://master:9870"
      - "SPARK_MASTER=spark://spark-master:7077"
      - "MASTER:=spark://spark-master:7077"
    depends_on:
      - spark-master
    networks:
      - hadoop_network


networks:
  hadoop_network:
    external: true
  